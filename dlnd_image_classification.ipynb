{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb1c1826a0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.astype('float32')\n",
    "    return x / 255.0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "    lb.fit(np.array([[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0]]))\n",
    "    return lb.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.placeholder(tf.float32, shape= [None, *image_shape], name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    input_shape = int(x_tensor.get_shape()[3])\n",
    "    weights = tf.Variable(tf.truncated_normal([*conv_ksize, input_shape, conv_num_outputs], dtype=tf.float32))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides= [1, *conv_strides, 1], padding=\"SAME\")\n",
    "    conv_layer = conv_layer + bias\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding=\"SAME\")\n",
    "    return conv_layer\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.reshape(x_tensor, [-1, x_tensor.get_shape().as_list()[1]*\n",
    "                                 x_tensor.get_shape().as_list()[2]*x_tensor.get_shape().as_list()[3]])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs, activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 10\n",
    "    conv_ksize = (3,3)\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (2, 2)\n",
    "    pool_strides = (2, 2)\n",
    "    conv_layer = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layer = tf.nn.dropout(conv_layer, keep_prob)\n",
    "    \n",
    "\n",
    "    \n",
    "    #   flatten(x_tensor)\n",
    "    flatten_layer = flatten(conv_layer)\n",
    "\n",
    "    \n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    num_outputs = 128\n",
    "    fully_connected = fully_conn(flatten_layer, num_outputs)\n",
    "    num_outputs1 = 64\n",
    "    fully_connected = fully_conn(fully_connected, num_outputs1)\n",
    "    num_outputs2 = 32\n",
    "    fully_connected = fully_conn(fully_connected, num_outputs2)\n",
    "    num_outputs3 = 10\n",
    "   \n",
    "    #   output(x_tensor, num_outputs)\n",
    "    output_layer = output(fully_connected, num_outputs3)\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "    acc = session.run(accuracy, feed_dict={x:valid_features, y:valid_labels, keep_prob:1.0})\n",
    "    print('Loss at {}'.format(loss), 'Validation Accuracy at {}'.format(acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 2.2612926959991455 Validation Accuracy at 0.18719998002052307\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 2.0979013442993164 Validation Accuracy at 0.27239999175071716\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 1.9174472093582153 Validation Accuracy at 0.3190000057220459\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 1.8121085166931152 Validation Accuracy at 0.37779995799064636\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 1.6938802003860474 Validation Accuracy at 0.3887999951839447\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 1.5398962497711182 Validation Accuracy at 0.40059998631477356\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 1.39851975440979 Validation Accuracy at 0.41419994831085205\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 1.326624870300293 Validation Accuracy at 0.42499998211860657\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 1.2611736059188843 Validation Accuracy at 0.4358000159263611\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 1.1597802639007568 Validation Accuracy at 0.447799950838089\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 1.1038458347320557 Validation Accuracy at 0.4501999616622925\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 0.9863119125366211 Validation Accuracy at 0.4551999866962433\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 0.929739773273468 Validation Accuracy at 0.4559999704360962\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 0.8453500866889954 Validation Accuracy at 0.46859994530677795\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 0.8114632964134216 Validation Accuracy at 0.4601999521255493\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 0.7579848766326904 Validation Accuracy at 0.4691999554634094\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 0.715431809425354 Validation Accuracy at 0.4691999852657318\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 0.645458459854126 Validation Accuracy at 0.4795999825000763\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 0.6223629713058472 Validation Accuracy at 0.48239994049072266\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 0.586448073387146 Validation Accuracy at 0.4841999411582947\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 0.5274107456207275 Validation Accuracy at 0.4817999601364136\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 0.48529085516929626 Validation Accuracy at 0.49300000071525574\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 0.4709300398826599 Validation Accuracy at 0.4931999444961548\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 0.4186069965362549 Validation Accuracy at 0.4991999864578247\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 0.38190600275993347 Validation Accuracy at 0.49959996342658997\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 0.3733054995536804 Validation Accuracy at 0.5007998943328857\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 0.343677818775177 Validation Accuracy at 0.5021998882293701\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 0.31768128275871277 Validation Accuracy at 0.502799928188324\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 0.29992830753326416 Validation Accuracy at 0.49939996004104614\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 0.2843315005302429 Validation Accuracy at 0.5073999166488647\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 0.2687009572982788 Validation Accuracy at 0.5137999057769775\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 0.23266687989234924 Validation Accuracy at 0.5025999546051025\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 0.23961511254310608 Validation Accuracy at 0.5067999362945557\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 0.21739542484283447 Validation Accuracy at 0.5019999742507935\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 0.20144876837730408 Validation Accuracy at 0.5083999037742615\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 0.19609716534614563 Validation Accuracy at 0.509399950504303\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 0.19488483667373657 Validation Accuracy at 0.5094000101089478\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 0.1594393253326416 Validation Accuracy at 0.5021999478340149\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 0.17275533080101013 Validation Accuracy at 0.5065999627113342\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 0.163918137550354 Validation Accuracy at 0.5059999227523804\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 0.1584160327911377 Validation Accuracy at 0.506399929523468\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 0.13771918416023254 Validation Accuracy at 0.5137999653816223\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 0.1361972987651825 Validation Accuracy at 0.5083999633789062\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 0.12880398333072662 Validation Accuracy at 0.511199951171875\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 0.12786972522735596 Validation Accuracy at 0.5107999444007874\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 0.11828812956809998 Validation Accuracy at 0.5135999917984009\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 0.1077127605676651 Validation Accuracy at 0.5077999830245972\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 0.1006016954779625 Validation Accuracy at 0.5067999362945557\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 0.09475266933441162 Validation Accuracy at 0.5069999694824219\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 0.08649644255638123 Validation Accuracy at 0.5069999694824219\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 0.08072645217180252 Validation Accuracy at 0.5051999092102051\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 0.07244359701871872 Validation Accuracy at 0.5057999491691589\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 0.0735657811164856 Validation Accuracy at 0.5121999979019165\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 0.08148735761642456 Validation Accuracy at 0.5061998963356018\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 0.06729453057050705 Validation Accuracy at 0.49859994649887085\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 0.06698682904243469 Validation Accuracy at 0.504599928855896\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 0.06142040714621544 Validation Accuracy at 0.5095999836921692\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 0.05841286480426788 Validation Accuracy at 0.5023999810218811\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 0.05517350509762764 Validation Accuracy at 0.5119999647140503\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 0.055047012865543365 Validation Accuracy at 0.5097999572753906\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 0.050035275518894196 Validation Accuracy at 0.4997999668121338\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 0.05115864425897598 Validation Accuracy at 0.5043999552726746\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 0.05491584911942482 Validation Accuracy at 0.5047999620437622\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 0.05101560056209564 Validation Accuracy at 0.4949999451637268\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 0.04653402417898178 Validation Accuracy at 0.49439990520477295\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 0.04579806327819824 Validation Accuracy at 0.503600001335144\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 0.040132761001586914 Validation Accuracy at 0.4975999593734741\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 0.043117620050907135 Validation Accuracy at 0.5011999607086182\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 0.037057504057884216 Validation Accuracy at 0.5061998963356018\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 0.04298623651266098 Validation Accuracy at 0.5029999613761902\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 0.03384403884410858 Validation Accuracy at 0.49599993228912354\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 0.03715451434254646 Validation Accuracy at 0.488599956035614\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 0.03575370833277702 Validation Accuracy at 0.4941999912261963\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 0.034535594284534454 Validation Accuracy at 0.5049999356269836\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 0.03230778127908707 Validation Accuracy at 0.4949999451637268\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 0.027916038408875465 Validation Accuracy at 0.5035999417304993\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 0.02882148139178753 Validation Accuracy at 0.5003999471664429\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 0.028551898896694183 Validation Accuracy at 0.5019999146461487\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 0.03322110325098038 Validation Accuracy at 0.4989999532699585\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 0.0280268806964159 Validation Accuracy at 0.49359995126724243\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 0.0218306016176939 Validation Accuracy at 0.5033999681472778\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 0.022601304575800896 Validation Accuracy at 0.5023999214172363\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 0.025837428867816925 Validation Accuracy at 0.4987999200820923\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 0.02261875942349434 Validation Accuracy at 0.4989999234676361\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 0.02416483499109745 Validation Accuracy at 0.501599907875061\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 0.02072276920080185 Validation Accuracy at 0.5062000155448914\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 0.02278231456875801 Validation Accuracy at 0.4989999234676361\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 0.016564305871725082 Validation Accuracy at 0.4989999234676361\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 0.01730465702712536 Validation Accuracy at 0.49939996004104614\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 0.014320885762572289 Validation Accuracy at 0.4989999532699585\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 0.019573723897337914 Validation Accuracy at 0.49519994854927063\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 0.017198078334331512 Validation Accuracy at 0.4943999648094177\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 0.016927102580666542 Validation Accuracy at 0.5055999755859375\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 0.014180449768900871 Validation Accuracy at 0.5025998950004578\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 0.013302305713295937 Validation Accuracy at 0.5049999952316284\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 0.011970343068242073 Validation Accuracy at 0.5013999342918396\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 0.01379803754389286 Validation Accuracy at 0.5077999234199524\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 0.013329321518540382 Validation Accuracy at 0.5019999742507935\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss at 0.010051248595118523 Validation Accuracy at 0.5069999694824219\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss at 0.01405488420277834 Validation Accuracy at 0.509399950504303\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 2.173184871673584 Validation Accuracy at 0.21320000290870667\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss at 1.8576257228851318 Validation Accuracy at 0.30299997329711914\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss at 1.7825870513916016 Validation Accuracy at 0.32339999079704285\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss at 1.7709273099899292 Validation Accuracy at 0.3601999878883362\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss at 1.6121848821640015 Validation Accuracy at 0.38840001821517944\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 1.832869052886963 Validation Accuracy at 0.40959998965263367\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss at 1.48883855342865 Validation Accuracy at 0.42139995098114014\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss at 1.4092650413513184 Validation Accuracy at 0.4197999835014343\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss at 1.5004504919052124 Validation Accuracy at 0.4472000002861023\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss at 1.3610813617706299 Validation Accuracy at 0.45979997515678406\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 1.5708791017532349 Validation Accuracy at 0.46859997510910034\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss at 1.256544589996338 Validation Accuracy at 0.4691999852657318\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss at 1.1486480236053467 Validation Accuracy at 0.46799999475479126\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss at 1.2596064805984497 Validation Accuracy at 0.4861999750137329\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss at 1.2458637952804565 Validation Accuracy at 0.4891999363899231\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 1.4051188230514526 Validation Accuracy at 0.4968000054359436\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss at 1.0771774053573608 Validation Accuracy at 0.49359995126724243\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss at 0.9975936412811279 Validation Accuracy at 0.49379992485046387\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss at 1.1099560260772705 Validation Accuracy at 0.5067999362945557\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss at 1.1642717123031616 Validation Accuracy at 0.5091999173164368\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 1.2770719528198242 Validation Accuracy at 0.5191999673843384\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss at 0.9412181377410889 Validation Accuracy at 0.5145999789237976\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss at 0.9077620506286621 Validation Accuracy at 0.5119999647140503\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss at 1.018660545349121 Validation Accuracy at 0.5199999213218689\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss at 1.089375615119934 Validation Accuracy at 0.5273998975753784\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 1.1864219903945923 Validation Accuracy at 0.5283999443054199\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss at 0.8502894043922424 Validation Accuracy at 0.5245999693870544\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss at 0.7976807951927185 Validation Accuracy at 0.5263999104499817\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss at 0.8847365975379944 Validation Accuracy at 0.5343999862670898\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss at 0.9397158622741699 Validation Accuracy at 0.5379998683929443\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 1.1019434928894043 Validation Accuracy at 0.5455999374389648\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss at 0.769005537033081 Validation Accuracy at 0.5365999341011047\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss at 0.7162944674491882 Validation Accuracy at 0.5417999625205994\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss at 0.7758797407150269 Validation Accuracy at 0.5509999394416809\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss at 0.8749788403511047 Validation Accuracy at 0.5575999021530151\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 1.0349383354187012 Validation Accuracy at 0.5507999658584595\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss at 0.6824361085891724 Validation Accuracy at 0.5535998940467834\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss at 0.6157830953598022 Validation Accuracy at 0.5511999130249023\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss at 0.7314432859420776 Validation Accuracy at 0.5593999624252319\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss at 0.7849761843681335 Validation Accuracy at 0.5591999292373657\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 0.9461194276809692 Validation Accuracy at 0.5577999353408813\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss at 0.650943398475647 Validation Accuracy at 0.5587999224662781\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss at 0.6107425093650818 Validation Accuracy at 0.5553998947143555\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss at 0.6662354469299316 Validation Accuracy at 0.5653998851776123\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss at 0.7479987144470215 Validation Accuracy at 0.5669999122619629\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 0.8738563060760498 Validation Accuracy at 0.5729999542236328\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss at 0.5822643637657166 Validation Accuracy at 0.5677999258041382\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss at 0.5843439102172852 Validation Accuracy at 0.5623999238014221\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss at 0.6065384149551392 Validation Accuracy at 0.5697999000549316\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss at 0.6772739887237549 Validation Accuracy at 0.5733999609947205\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 0.8064392805099487 Validation Accuracy at 0.582399845123291\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss at 0.53010094165802 Validation Accuracy at 0.5649999380111694\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss at 0.5674732327461243 Validation Accuracy at 0.5653999447822571\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss at 0.5950722694396973 Validation Accuracy at 0.5835999250411987\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss at 0.6052362322807312 Validation Accuracy at 0.5795998573303223\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 0.7822180390357971 Validation Accuracy at 0.5829999446868896\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss at 0.5299279093742371 Validation Accuracy at 0.5673999190330505\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss at 0.49713775515556335 Validation Accuracy at 0.5775998830795288\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss at 0.5113378763198853 Validation Accuracy at 0.5817998647689819\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss at 0.5496963262557983 Validation Accuracy at 0.5849999189376831\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 0.6986191272735596 Validation Accuracy at 0.5877999067306519\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss at 0.46092119812965393 Validation Accuracy at 0.5725999474525452\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss at 0.4701189398765564 Validation Accuracy at 0.578999936580658\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss at 0.46608203649520874 Validation Accuracy at 0.5827999114990234\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss at 0.535266637802124 Validation Accuracy at 0.5977998971939087\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 0.6831938028335571 Validation Accuracy at 0.5859999656677246\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss at 0.44543373584747314 Validation Accuracy at 0.5769999027252197\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss at 0.41381943225860596 Validation Accuracy at 0.5767999291419983\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss at 0.4565792381763458 Validation Accuracy at 0.5951999425888062\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss at 0.45160970091819763 Validation Accuracy at 0.5895999073982239\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 0.6303600072860718 Validation Accuracy at 0.5961999297142029\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss at 0.39542195200920105 Validation Accuracy at 0.5765999555587769\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss at 0.4176959991455078 Validation Accuracy at 0.5855998992919922\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss at 0.3999320864677429 Validation Accuracy at 0.5917999148368835\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss at 0.4331609308719635 Validation Accuracy at 0.6017999053001404\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 0.6155036091804504 Validation Accuracy at 0.5955999493598938\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss at 0.42155155539512634 Validation Accuracy at 0.5873998999595642\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss at 0.3809126019477844 Validation Accuracy at 0.5907999277114868\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss at 0.41538193821907043 Validation Accuracy at 0.5939998626708984\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss at 0.4022076725959778 Validation Accuracy at 0.5921999216079712\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 0.5634037852287292 Validation Accuracy at 0.5997998714447021\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss at 0.3855462670326233 Validation Accuracy at 0.5741999745368958\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss at 0.3456459045410156 Validation Accuracy at 0.5947999358177185\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss at 0.35912811756134033 Validation Accuracy at 0.5963999629020691\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss at 0.3827638626098633 Validation Accuracy at 0.6017999053001404\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 0.5362962484359741 Validation Accuracy at 0.5977998971939087\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss at 0.34025025367736816 Validation Accuracy at 0.5869998931884766\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss at 0.3339291214942932 Validation Accuracy at 0.5911999344825745\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss at 0.34867268800735474 Validation Accuracy at 0.5957999229431152\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss at 0.3656597137451172 Validation Accuracy at 0.6017998456954956\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 0.5178297758102417 Validation Accuracy at 0.6043999195098877\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss at 0.34020376205444336 Validation Accuracy at 0.5771999359130859\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss at 0.29460909962654114 Validation Accuracy at 0.5993999242782593\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss at 0.32411354780197144 Validation Accuracy at 0.5989999175071716\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss at 0.3623959422111511 Validation Accuracy at 0.6029998660087585\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 0.4771249294281006 Validation Accuracy at 0.6007998585700989\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss at 0.3150970935821533 Validation Accuracy at 0.5815998911857605\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss at 0.2961065173149109 Validation Accuracy at 0.5983999371528625\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss at 0.31623372435569763 Validation Accuracy at 0.6009999513626099\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss at 0.33783578872680664 Validation Accuracy at 0.6017999053001404\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 0.47055530548095703 Validation Accuracy at 0.6055998802185059\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss at 0.30043646693229675 Validation Accuracy at 0.5949999094009399\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss at 0.2908807396888733 Validation Accuracy at 0.599399983882904\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss at 0.3044874370098114 Validation Accuracy at 0.6043999195098877\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss at 0.3216082453727722 Validation Accuracy at 0.6047998666763306\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 0.4520668387413025 Validation Accuracy at 0.6035999059677124\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss at 0.27363312244415283 Validation Accuracy at 0.5937999486923218\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss at 0.23905538022518158 Validation Accuracy at 0.6011999845504761\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss at 0.29784828424453735 Validation Accuracy at 0.6077998876571655\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss at 0.3021601438522339 Validation Accuracy at 0.6005998849868774\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 0.4252949059009552 Validation Accuracy at 0.6093999147415161\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss at 0.25581419467926025 Validation Accuracy at 0.586199939250946\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss at 0.24355167150497437 Validation Accuracy at 0.5995998978614807\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss at 0.2817944288253784 Validation Accuracy at 0.6023999452590942\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss at 0.27471011877059937 Validation Accuracy at 0.6093999743461609\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 0.3943841755390167 Validation Accuracy at 0.6085999011993408\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss at 0.256696879863739 Validation Accuracy at 0.5977998971939087\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss at 0.22650882601737976 Validation Accuracy at 0.6037998795509338\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss at 0.2698749303817749 Validation Accuracy at 0.606999933719635\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss at 0.2758919298648834 Validation Accuracy at 0.6013998985290527\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 0.37716928124427795 Validation Accuracy at 0.6085999011993408\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss at 0.236224964261055 Validation Accuracy at 0.5991998910903931\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss at 0.20652036368846893 Validation Accuracy at 0.6029999256134033\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss at 0.26405179500579834 Validation Accuracy at 0.6057999134063721\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss at 0.2623116075992584 Validation Accuracy at 0.6035999059677124\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 0.3630426526069641 Validation Accuracy at 0.6141999363899231\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss at 0.23335132002830505 Validation Accuracy at 0.5961998701095581\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss at 0.20190531015396118 Validation Accuracy at 0.6047999262809753\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss at 0.25868886709213257 Validation Accuracy at 0.608799934387207\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss at 0.25406354665756226 Validation Accuracy at 0.6037999391555786\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 0.34017837047576904 Validation Accuracy at 0.6119999289512634\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss at 0.2323920726776123 Validation Accuracy at 0.601599931716919\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss at 0.1992015540599823 Validation Accuracy at 0.6049998998641968\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss at 0.2429288923740387 Validation Accuracy at 0.6075998544692993\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss at 0.22735784947872162 Validation Accuracy at 0.6041998863220215\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 0.3572663366794586 Validation Accuracy at 0.6133999228477478\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss at 0.19631151854991913 Validation Accuracy at 0.5977998971939087\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss at 0.20367927849292755 Validation Accuracy at 0.600399911403656\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss at 0.2481054812669754 Validation Accuracy at 0.6107998490333557\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss at 0.24298200011253357 Validation Accuracy at 0.607999861240387\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 0.34699034690856934 Validation Accuracy at 0.6135998964309692\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss at 0.19970671832561493 Validation Accuracy at 0.6049999594688416\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss at 0.1783628612756729 Validation Accuracy at 0.6073999404907227\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss at 0.2337321639060974 Validation Accuracy at 0.6099998950958252\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss at 0.2330099642276764 Validation Accuracy at 0.6093999147415161\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 0.3554215431213379 Validation Accuracy at 0.6077999472618103\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss at 0.19431078433990479 Validation Accuracy at 0.5985999703407288\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss at 0.18560361862182617 Validation Accuracy at 0.6073999404907227\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss at 0.21962015330791473 Validation Accuracy at 0.6119998693466187\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss at 0.2280711978673935 Validation Accuracy at 0.6107999682426453\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 0.3433723747730255 Validation Accuracy at 0.6107999086380005\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss at 0.18322648108005524 Validation Accuracy at 0.6009999513626099\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss at 0.17419761419296265 Validation Accuracy at 0.6067999601364136\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss at 0.20995017886161804 Validation Accuracy at 0.6223998665809631\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss at 0.22054722905158997 Validation Accuracy at 0.6097999215126038\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 0.32431381940841675 Validation Accuracy at 0.6149999499320984\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss at 0.19248691201210022 Validation Accuracy at 0.6115999221801758\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss at 0.15514183044433594 Validation Accuracy at 0.6037999391555786\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss at 0.21633999049663544 Validation Accuracy at 0.6171998977661133\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss at 0.18914997577667236 Validation Accuracy at 0.6115999221801758\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 0.3084705173969269 Validation Accuracy at 0.6177998781204224\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss at 0.16382190585136414 Validation Accuracy at 0.6083999872207642\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss at 0.1518010050058365 Validation Accuracy at 0.6121999025344849\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss at 0.20381852984428406 Validation Accuracy at 0.6103999018669128\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss at 0.18828438222408295 Validation Accuracy at 0.6145999431610107\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 0.2894669771194458 Validation Accuracy at 0.6123999357223511\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss at 0.150767520070076 Validation Accuracy at 0.612799882888794\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss at 0.1341426819562912 Validation Accuracy at 0.6123999357223511\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss at 0.21123841404914856 Validation Accuracy at 0.6147999167442322\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss at 0.1770644187927246 Validation Accuracy at 0.6121999025344849\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 0.2760557532310486 Validation Accuracy at 0.6197998523712158\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss at 0.14398379623889923 Validation Accuracy at 0.605199933052063\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss at 0.1421249359846115 Validation Accuracy at 0.6099998950958252\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss at 0.22070074081420898 Validation Accuracy at 0.6169999241828918\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss at 0.15397082269191742 Validation Accuracy at 0.6089999079704285\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 0.26870131492614746 Validation Accuracy at 0.6187998652458191\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss at 0.14159336686134338 Validation Accuracy at 0.6055998802185059\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss at 0.13726042211055756 Validation Accuracy at 0.6083998680114746\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss at 0.20065677165985107 Validation Accuracy at 0.6143999099731445\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss at 0.15860943496227264 Validation Accuracy at 0.6119998693466187\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 0.24770528078079224 Validation Accuracy at 0.615399956703186\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss at 0.12459932267665863 Validation Accuracy at 0.6013998985290527\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss at 0.14637480676174164 Validation Accuracy at 0.6065999269485474\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss at 0.20407330989837646 Validation Accuracy at 0.6147998571395874\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss at 0.16708649694919586 Validation Accuracy at 0.6173999309539795\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 0.25391504168510437 Validation Accuracy at 0.6183999180793762\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss at 0.1215546578168869 Validation Accuracy at 0.6051998734474182\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss at 0.13773779571056366 Validation Accuracy at 0.6107999086380005\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss at 0.16851142048835754 Validation Accuracy at 0.6109999418258667\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss at 0.17265889048576355 Validation Accuracy at 0.6133999824523926\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 0.2532777190208435 Validation Accuracy at 0.6189998984336853\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss at 0.13115215301513672 Validation Accuracy at 0.6071999073028564\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss at 0.1360221654176712 Validation Accuracy at 0.6095999479293823\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss at 0.1779816746711731 Validation Accuracy at 0.616399884223938\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss at 0.15924794971942902 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 0.24151360988616943 Validation Accuracy at 0.6189998984336853\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss at 0.11863569915294647 Validation Accuracy at 0.6105998754501343\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss at 0.13596460223197937 Validation Accuracy at 0.6103999018669128\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss at 0.17306602001190186 Validation Accuracy at 0.6139999032020569\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss at 0.14156898856163025 Validation Accuracy at 0.6139999032020569\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 0.22056277096271515 Validation Accuracy at 0.6173999309539795\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss at 0.12243016064167023 Validation Accuracy at 0.614799976348877\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss at 0.12970739603042603 Validation Accuracy at 0.6117998957633972\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss at 0.1684994250535965 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss at 0.14751628041267395 Validation Accuracy at 0.6121999025344849\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 0.2340877503156662 Validation Accuracy at 0.6203998923301697\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss at 0.11114619672298431 Validation Accuracy at 0.6079999208450317\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss at 0.11502181738615036 Validation Accuracy at 0.6111998558044434\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss at 0.1698770523071289 Validation Accuracy at 0.6167998909950256\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss at 0.15422211587429047 Validation Accuracy at 0.6123999357223511\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 0.22719597816467285 Validation Accuracy at 0.6151999235153198\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss at 0.10487905144691467 Validation Accuracy at 0.6051998734474182\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss at 0.11877317726612091 Validation Accuracy at 0.6131998896598816\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss at 0.1665513813495636 Validation Accuracy at 0.6159998774528503\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss at 0.13970552384853363 Validation Accuracy at 0.6167998909950256\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 0.22329360246658325 Validation Accuracy at 0.6195999383926392\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss at 0.10578981041908264 Validation Accuracy at 0.6107999086380005\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss at 0.11443676054477692 Validation Accuracy at 0.6173998713493347\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss at 0.15740053355693817 Validation Accuracy at 0.619999885559082\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss at 0.138624906539917 Validation Accuracy at 0.614799976348877\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 0.21261243522167206 Validation Accuracy at 0.6203998327255249\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss at 0.09529411792755127 Validation Accuracy at 0.6117998957633972\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss at 0.11806822568178177 Validation Accuracy at 0.6101999282836914\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss at 0.15880003571510315 Validation Accuracy at 0.6207998991012573\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss at 0.1324940174818039 Validation Accuracy at 0.6137998700141907\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 0.21246427297592163 Validation Accuracy at 0.6161998510360718\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss at 0.10985534638166428 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss at 0.11908036470413208 Validation Accuracy at 0.60999995470047\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss at 0.14000913500785828 Validation Accuracy at 0.6207998991012573\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss at 0.12131576985120773 Validation Accuracy at 0.6147999167442322\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 0.2099713385105133 Validation Accuracy at 0.6219999194145203\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss at 0.09740446507930756 Validation Accuracy at 0.6083999872207642\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss at 0.12131334841251373 Validation Accuracy at 0.6131999492645264\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss at 0.15298888087272644 Validation Accuracy at 0.6213999390602112\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss at 0.11822091788053513 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 0.17647206783294678 Validation Accuracy at 0.6195999383926392\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss at 0.09233902394771576 Validation Accuracy at 0.608199954032898\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss at 0.1112942099571228 Validation Accuracy at 0.6113999485969543\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss at 0.13990715146064758 Validation Accuracy at 0.6137999296188354\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss at 0.1246432363986969 Validation Accuracy at 0.6169998645782471\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 0.17889127135276794 Validation Accuracy at 0.6171999573707581\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss at 0.09138065576553345 Validation Accuracy at 0.6097999215126038\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss at 0.11188699305057526 Validation Accuracy at 0.6151999235153198\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss at 0.12716004252433777 Validation Accuracy at 0.619399905204773\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss at 0.11231300979852676 Validation Accuracy at 0.6123999357223511\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 0.20595595240592957 Validation Accuracy at 0.6179998517036438\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss at 0.09844239056110382 Validation Accuracy at 0.6091998815536499\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss at 0.09604395180940628 Validation Accuracy at 0.6159999370574951\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss at 0.11332067102193832 Validation Accuracy at 0.6225999593734741\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss at 0.10020186007022858 Validation Accuracy at 0.6151999235153198\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 0.19267211854457855 Validation Accuracy at 0.6179999113082886\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss at 0.10218580812215805 Validation Accuracy at 0.6063998937606812\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss at 0.0935603529214859 Validation Accuracy at 0.6151999235153198\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss at 0.09606093168258667 Validation Accuracy at 0.6201999187469482\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss at 0.1147412657737732 Validation Accuracy at 0.6109999418258667\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 0.18112997710704803 Validation Accuracy at 0.6195999383926392\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss at 0.09219256043434143 Validation Accuracy at 0.6133999228477478\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss at 0.1011791005730629 Validation Accuracy at 0.6201998591423035\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss at 0.11774222552776337 Validation Accuracy at 0.6191998720169067\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss at 0.10895399749279022 Validation Accuracy at 0.6179999113082886\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 0.1792706549167633 Validation Accuracy at 0.6175999641418457\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss at 0.08098543435335159 Validation Accuracy at 0.6135998964309692\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss at 0.08850192278623581 Validation Accuracy at 0.6183999180793762\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss at 0.11152234673500061 Validation Accuracy at 0.6269998550415039\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss at 0.1054147258400917 Validation Accuracy at 0.6235998868942261\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 0.16104988753795624 Validation Accuracy at 0.6201999187469482\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss at 0.08901918679475784 Validation Accuracy at 0.6089999079704285\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss at 0.10824104398488998 Validation Accuracy at 0.6249998807907104\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss at 0.09917297214269638 Validation Accuracy at 0.6215999126434326\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss at 0.10729202628135681 Validation Accuracy at 0.6143999099731445\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 0.1590743511915207 Validation Accuracy at 0.6163999438285828\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss at 0.0769701674580574 Validation Accuracy at 0.6149998903274536\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss at 0.09803495556116104 Validation Accuracy at 0.6155999898910522\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss at 0.1043720468878746 Validation Accuracy at 0.6231998205184937\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss at 0.09746546298265457 Validation Accuracy at 0.6195998787879944\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 0.15788021683692932 Validation Accuracy at 0.6213999390602112\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss at 0.06883952766656876 Validation Accuracy at 0.6089999079704285\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss at 0.097315713763237 Validation Accuracy at 0.6245998740196228\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss at 0.10391709208488464 Validation Accuracy at 0.6245999336242676\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss at 0.09869374334812164 Validation Accuracy at 0.6153998970985413\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 0.1484246402978897 Validation Accuracy at 0.622999906539917\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss at 0.07786603271961212 Validation Accuracy at 0.6183999180793762\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss at 0.10480880737304688 Validation Accuracy at 0.6171998977661133\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss at 0.10136938840150833 Validation Accuracy at 0.6241998672485352\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss at 0.0958331972360611 Validation Accuracy at 0.6183999180793762\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 0.15247711539268494 Validation Accuracy at 0.6203998923301697\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss at 0.07120595872402191 Validation Accuracy at 0.6131999492645264\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss at 0.09272365272045135 Validation Accuracy at 0.621199905872345\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss at 0.09414243698120117 Validation Accuracy at 0.6269999146461487\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss at 0.09933684021234512 Validation Accuracy at 0.6219999194145203\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 0.1569807231426239 Validation Accuracy at 0.6161999106407166\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss at 0.07000182569026947 Validation Accuracy at 0.6191999316215515\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss at 0.0856139212846756 Validation Accuracy at 0.6185999512672424\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss at 0.09917251765727997 Validation Accuracy at 0.6207999587059021\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss at 0.0880722925066948 Validation Accuracy at 0.6201999187469482\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 0.13854371011257172 Validation Accuracy at 0.6173999309539795\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss at 0.07407211512327194 Validation Accuracy at 0.609799861907959\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss at 0.09450055658817291 Validation Accuracy at 0.6137998700141907\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss at 0.07738666236400604 Validation Accuracy at 0.6231999397277832\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss at 0.09654491394758224 Validation Accuracy at 0.620199978351593\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 0.13695970177650452 Validation Accuracy at 0.622999906539917\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss at 0.06589029729366302 Validation Accuracy at 0.6129999160766602\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss at 0.08912362903356552 Validation Accuracy at 0.6179999113082886\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss at 0.08451097458600998 Validation Accuracy at 0.6253999471664429\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss at 0.08475904166698456 Validation Accuracy at 0.6223998665809631\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 0.1283736675977707 Validation Accuracy at 0.6171998977661133\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss at 0.0704561322927475 Validation Accuracy at 0.6123998761177063\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss at 0.09286037087440491 Validation Accuracy at 0.619999885559082\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss at 0.08756732195615768 Validation Accuracy at 0.6239998936653137\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss at 0.08017970621585846 Validation Accuracy at 0.6205999255180359\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 0.13808029890060425 Validation Accuracy at 0.6231998205184937\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss at 0.059819698333740234 Validation Accuracy at 0.6139999032020569\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss at 0.0877811461687088 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss at 0.07762515544891357 Validation Accuracy at 0.624799907207489\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss at 0.07986170053482056 Validation Accuracy at 0.6231998801231384\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 0.12810419499874115 Validation Accuracy at 0.6187999248504639\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss at 0.05897293612360954 Validation Accuracy at 0.612799882888794\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss at 0.07672957330942154 Validation Accuracy at 0.6181999444961548\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss at 0.08260594308376312 Validation Accuracy at 0.6183999180793762\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss at 0.07880473136901855 Validation Accuracy at 0.6111999154090881\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 0.13469958305358887 Validation Accuracy at 0.6185998916625977\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss at 0.06013276427984238 Validation Accuracy at 0.6137998700141907\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss at 0.0722004696726799 Validation Accuracy at 0.6225998997688293\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss at 0.08205261826515198 Validation Accuracy at 0.6207998991012573\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss at 0.07849743962287903 Validation Accuracy at 0.6159998774528503\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 0.12845122814178467 Validation Accuracy at 0.619399905204773\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss at 0.062468621879816055 Validation Accuracy at 0.6075999140739441\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss at 0.0736902505159378 Validation Accuracy at 0.6177998781204224\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss at 0.08301757276058197 Validation Accuracy at 0.6259998679161072\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss at 0.07996818423271179 Validation Accuracy at 0.6113998889923096\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 0.13299456238746643 Validation Accuracy at 0.6159998774528503\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss at 0.05868690833449364 Validation Accuracy at 0.6067999005317688\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss at 0.0722176730632782 Validation Accuracy at 0.6197999715805054\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss at 0.07851190865039825 Validation Accuracy at 0.6219999194145203\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss at 0.06465939432382584 Validation Accuracy at 0.6189998388290405\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 0.1401008814573288 Validation Accuracy at 0.6197999119758606\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss at 0.05594061687588692 Validation Accuracy at 0.6097999215126038\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss at 0.0700511634349823 Validation Accuracy at 0.6177998781204224\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss at 0.0636468380689621 Validation Accuracy at 0.6175999641418457\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss at 0.06922391057014465 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 0.12595997750759125 Validation Accuracy at 0.614599883556366\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss at 0.05621059611439705 Validation Accuracy at 0.6083998680114746\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss at 0.07285674661397934 Validation Accuracy at 0.6167998909950256\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss at 0.06409233063459396 Validation Accuracy at 0.6227999329566956\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss at 0.0836033970117569 Validation Accuracy at 0.6159999370574951\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 0.11948976665735245 Validation Accuracy at 0.614599883556366\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss at 0.05704234540462494 Validation Accuracy at 0.6169999241828918\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss at 0.07927980273962021 Validation Accuracy at 0.6173999905586243\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss at 0.06555290520191193 Validation Accuracy at 0.61819988489151\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss at 0.07227298617362976 Validation Accuracy at 0.6135998964309692\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 0.10762432217597961 Validation Accuracy at 0.6133999228477478\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss at 0.05379965156316757 Validation Accuracy at 0.6113998889923096\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss at 0.06857474893331528 Validation Accuracy at 0.6187999248504639\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss at 0.0586189329624176 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss at 0.07752272486686707 Validation Accuracy at 0.6195998787879944\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 0.11374862492084503 Validation Accuracy at 0.6203999519348145\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss at 0.05138711631298065 Validation Accuracy at 0.6077999472618103\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss at 0.07460854202508926 Validation Accuracy at 0.6169999241828918\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss at 0.05989179015159607 Validation Accuracy at 0.6217999458312988\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss at 0.06144346296787262 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 0.1154775619506836 Validation Accuracy at 0.6239998936653137\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss at 0.05155973136425018 Validation Accuracy at 0.6115999221801758\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss at 0.05942267179489136 Validation Accuracy at 0.6221998929977417\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss at 0.06481367349624634 Validation Accuracy at 0.6201999187469482\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss at 0.06264866888523102 Validation Accuracy at 0.6227998733520508\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 0.09942284971475601 Validation Accuracy at 0.61819988489151\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss at 0.04749518260359764 Validation Accuracy at 0.6129999756813049\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss at 0.06537239253520966 Validation Accuracy at 0.6151999235153198\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss at 0.06456051766872406 Validation Accuracy at 0.6195998787879944\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss at 0.06439371407032013 Validation Accuracy at 0.6213998794555664\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 0.09638691693544388 Validation Accuracy at 0.6153998970985413\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss at 0.04939529299736023 Validation Accuracy at 0.6137998700141907\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss at 0.06103602424263954 Validation Accuracy at 0.6247998476028442\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss at 0.05724985525012016 Validation Accuracy at 0.6253998875617981\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss at 0.0658905953168869 Validation Accuracy at 0.6169999241828918\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 0.09788250923156738 Validation Accuracy at 0.6205998659133911\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss at 0.05108354240655899 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss at 0.0526871420443058 Validation Accuracy at 0.6205999255180359\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss at 0.05660659819841385 Validation Accuracy at 0.6215998530387878\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss at 0.05770648270845413 Validation Accuracy at 0.6233999133110046\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 0.09614458680152893 Validation Accuracy at 0.6179999113082886\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss at 0.040801431983709335 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss at 0.05153699964284897 Validation Accuracy at 0.6245998740196228\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss at 0.05595923215150833 Validation Accuracy at 0.6237999200820923\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss at 0.06583850085735321 Validation Accuracy at 0.6209999322891235\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 0.09318859875202179 Validation Accuracy at 0.6147999167442322\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss at 0.050964757800102234 Validation Accuracy at 0.6167998909950256\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss at 0.05316458269953728 Validation Accuracy at 0.6185998916625977\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss at 0.05356530100107193 Validation Accuracy at 0.6201999187469482\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss at 0.06316287070512772 Validation Accuracy at 0.6209998726844788\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 0.0931677296757698 Validation Accuracy at 0.623999834060669\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss at 0.044154293835163116 Validation Accuracy at 0.6179999113082886\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss at 0.04845539852976799 Validation Accuracy at 0.6195999383926392\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss at 0.061640579253435135 Validation Accuracy at 0.619399905204773\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss at 0.06333701312541962 Validation Accuracy at 0.6219999194145203\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 0.09663084149360657 Validation Accuracy at 0.6197999119758606\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss at 0.04810892790555954 Validation Accuracy at 0.611799955368042\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss at 0.05096700042486191 Validation Accuracy at 0.6213999390602112\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss at 0.0534340925514698 Validation Accuracy at 0.6255998611450195\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss at 0.06119799241423607 Validation Accuracy at 0.6285998821258545\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 0.09331168234348297 Validation Accuracy at 0.619399905204773\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss at 0.047693945467472076 Validation Accuracy at 0.6225998997688293\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss at 0.047476138919591904 Validation Accuracy at 0.619999885559082\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss at 0.05983719974756241 Validation Accuracy at 0.6231999397277832\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss at 0.05977585166692734 Validation Accuracy at 0.6195998787879944\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 0.0828327015042305 Validation Accuracy at 0.6253998875617981\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss at 0.045979276299476624 Validation Accuracy at 0.6155999302864075\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss at 0.04630885273218155 Validation Accuracy at 0.6223999261856079\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss at 0.04947439953684807 Validation Accuracy at 0.621199905872345\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss at 0.05822455883026123 Validation Accuracy at 0.6231999397277832\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 0.0808856338262558 Validation Accuracy at 0.6263999342918396\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss at 0.04040359705686569 Validation Accuracy at 0.6185998916625977\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss at 0.04734737426042557 Validation Accuracy at 0.6255998611450195\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss at 0.051007941365242004 Validation Accuracy at 0.6231999397277832\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss at 0.05280200392007828 Validation Accuracy at 0.6209999322891235\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 0.08447673171758652 Validation Accuracy at 0.6249998807907104\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss at 0.038801971822977066 Validation Accuracy at 0.6149998903274536\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss at 0.04749272018671036 Validation Accuracy at 0.6243998408317566\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss at 0.04835093393921852 Validation Accuracy at 0.621199905872345\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss at 0.05002504587173462 Validation Accuracy at 0.6241998672485352\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 0.08056409657001495 Validation Accuracy at 0.6177998781204224\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss at 0.04078066349029541 Validation Accuracy at 0.6131998896598816\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss at 0.04469403624534607 Validation Accuracy at 0.6201999187469482\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss at 0.043638043105602264 Validation Accuracy at 0.6213998794555664\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss at 0.04815216362476349 Validation Accuracy at 0.6191999316215515\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 0.08092091232538223 Validation Accuracy at 0.6155998706817627\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss at 0.044921498745679855 Validation Accuracy at 0.6139999032020569\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss at 0.0469573438167572 Validation Accuracy at 0.6195998787879944\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss at 0.049641840159893036 Validation Accuracy at 0.6151999235153198\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss at 0.04562103748321533 Validation Accuracy at 0.6179998517036438\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 0.07246032357215881 Validation Accuracy at 0.6207998991012573\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss at 0.045778270810842514 Validation Accuracy at 0.6159999370574951\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss at 0.04596344009041786 Validation Accuracy at 0.6201998591423035\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss at 0.044913437217473984 Validation Accuracy at 0.6235998868942261\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss at 0.04861951246857643 Validation Accuracy at 0.6163999438285828\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 0.08825981616973877 Validation Accuracy at 0.621199905872345\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss at 0.039794087409973145 Validation Accuracy at 0.6169999241828918\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss at 0.044489555060863495 Validation Accuracy at 0.6211999654769897\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss at 0.04641101509332657 Validation Accuracy at 0.622999906539917\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss at 0.042920440435409546 Validation Accuracy at 0.6215998530387878\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 0.0755196139216423 Validation Accuracy at 0.6199999451637268\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss at 0.030214913189411163 Validation Accuracy at 0.6167999505996704\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss at 0.03919009119272232 Validation Accuracy at 0.621799886226654\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss at 0.04485248774290085 Validation Accuracy at 0.6241999268531799\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss at 0.04388329014182091 Validation Accuracy at 0.6233999133110046\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 0.06412935256958008 Validation Accuracy at 0.6167999505996704\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss at 0.03948447108268738 Validation Accuracy at 0.6107999086380005\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss at 0.04285309836268425 Validation Accuracy at 0.6159999370574951\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss at 0.04293029010295868 Validation Accuracy at 0.6233999133110046\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss at 0.042292095720767975 Validation Accuracy at 0.6169999241828918\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 0.0704130232334137 Validation Accuracy at 0.6141998767852783\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss at 0.04141749441623688 Validation Accuracy at 0.6159999370574951\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss at 0.04221721366047859 Validation Accuracy at 0.6189998984336853\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss at 0.040567152202129364 Validation Accuracy at 0.6241999268531799\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss at 0.03925589472055435 Validation Accuracy at 0.621199905872345\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 0.06908247619867325 Validation Accuracy at 0.6111999154090881\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss at 0.039090778678655624 Validation Accuracy at 0.6057998538017273\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss at 0.035366300493478775 Validation Accuracy at 0.6249998807907104\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss at 0.04009557515382767 Validation Accuracy at 0.6173999309539795\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss at 0.03947672247886658 Validation Accuracy at 0.6215999126434326\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 0.06300479173660278 Validation Accuracy at 0.6183999180793762\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss at 0.033675678074359894 Validation Accuracy at 0.6131998896598816\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss at 0.034433599561452866 Validation Accuracy at 0.6223999261856079\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss at 0.045560285449028015 Validation Accuracy at 0.6187999248504639\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss at 0.0471888966858387 Validation Accuracy at 0.6177998781204224\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 0.06728533655405045 Validation Accuracy at 0.611799955368042\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss at 0.033639244735240936 Validation Accuracy at 0.6175999045372009\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss at 0.035135142505168915 Validation Accuracy at 0.6167998909950256\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss at 0.043144430965185165 Validation Accuracy at 0.6261999011039734\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss at 0.04335226118564606 Validation Accuracy at 0.616399884223938\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 0.0796196460723877 Validation Accuracy at 0.6139999032020569\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss at 0.03545302897691727 Validation Accuracy at 0.6133999228477478\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss at 0.0365578830242157 Validation Accuracy at 0.6219999194145203\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss at 0.048214610666036606 Validation Accuracy at 0.6273998618125916\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss at 0.04784823954105377 Validation Accuracy at 0.613599956035614\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 0.07803778350353241 Validation Accuracy at 0.616399884223938\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss at 0.03451002016663551 Validation Accuracy at 0.612799882888794\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss at 0.03219008445739746 Validation Accuracy at 0.6205999255180359\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss at 0.036740101873874664 Validation Accuracy at 0.6189998984336853\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss at 0.036991946399211884 Validation Accuracy at 0.6179999113082886\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 0.06390474736690521 Validation Accuracy at 0.6169998645782471\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss at 0.03745248541235924 Validation Accuracy at 0.6157999038696289\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss at 0.03583540767431259 Validation Accuracy at 0.6225998401641846\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss at 0.0327749103307724 Validation Accuracy at 0.6169999241828918\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss at 0.034484829753637314 Validation Accuracy at 0.6183999180793762\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 0.06790454685688019 Validation Accuracy at 0.6167999505996704\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss at 0.03916839882731438 Validation Accuracy at 0.6185999512672424\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss at 0.03539475426077843 Validation Accuracy at 0.6195999383926392\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss at 0.03629351407289505 Validation Accuracy at 0.6191999316215515\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss at 0.03772290423512459 Validation Accuracy at 0.6183999180793762\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss at 0.08263435959815979 Validation Accuracy at 0.6163999438285828\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss at 0.03518915921449661 Validation Accuracy at 0.619999885559082\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss at 0.034881122410297394 Validation Accuracy at 0.622999906539917\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss at 0.040337949991226196 Validation Accuracy at 0.6275998950004578\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss at 0.03119698166847229 Validation Accuracy at 0.6209999322891235\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss at 0.07690557837486267 Validation Accuracy at 0.6197999119758606\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss at 0.03254207223653793 Validation Accuracy at 0.6161998510360718\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss at 0.03153578191995621 Validation Accuracy at 0.6225998997688293\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss at 0.029713822528719902 Validation Accuracy at 0.6237999200820923\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss at 0.04215167090296745 Validation Accuracy at 0.6189998984336853\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.62275390625\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIiEQKjkVEREREIgXHIiIiIiLR/wcRZ0EmqLlIHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbab8627710>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
